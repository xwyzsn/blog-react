---
link: https://blog.csdn.net/xwyzsn/article/details/113408218
title: 人工智能实验（A*，BP网络）
description: 人工智能实验（A*，BP）实验一 A*算法一、实验目的：熟悉和掌握启发式搜索的定义、估价函数和算法过程，并利用A*算法求解N数码难题，理解求解流程和搜索顺序。二、实验原理：A算法是一种启发式图搜索算法，其特点在于对估价函数的定义上。对于一般的启发式图搜索，总是选择估价函数f值最小的节点作为扩展节点。因此，f是根据需要找到一条最小代价路径的观点来估算节点的，所以，可考虑每个节点n的估价函数值为两个分量：从起始节点到节点n的实际代价以及从节点n*到达目标节点的估价代价。三、实验内容：1    参考
keywords: 人工智能实验（A，BP）
author: Xwyzsn Csdn认证博客专家 Csdn认证企业博客 码龄2年 暂无认证
date: 2021-01-31T16:03:00.000Z
publisher: null
catalog:project
stats: paragraph=96 sentences=152, words=1122
---
# 人工智能实验（A*，BP）

## 实验一 A*算法

**一、实验目的：**

熟悉和掌握启发式搜索的定义、估价函数和算法过程，并利用A*算法求解N数码难题，理解求解流程和搜索顺序。

**二、实验原理：**

A _算法是一种启发式图搜索算法，其特点在于对估价函数的定义上。对于一般的启发式图搜索，总是选择估价函数_f _值最小的节点作为扩展节点。因此，f 是根据需要找到一条最小代价路径的观点来估算节点的，所以，可考虑每个节点_n _的估价函数值为两个分量：从起始节点到节点_n _的实际代价以及从节点_n*到达目标节点的估价代价。

**三、实验内容：**

1 参考A _算法核心代码，以8数码问题为例实现A_算法的求解程序（编程语言不限），要求设计两种不同的估价函数。

估价函数1:代价函数为扩展的层数,启发函数为数码中不在位的数字的个数。

估价函数2：代价函数位扩展的层数，启发函数为由当前状态当目标状态所有节点需要移动的次数，即曼哈顿距离。

2 在求解8数码问题的A*算法程序中，设置相同的初始状态和目标状态，针对不同的估价函数，求得问题的解，并比较它们对搜索算法性能的影响，包括扩展节点数、生成节点数等。

算法流程图:

![](https://img-blog.csdnimg.cn/img_convert/db509f61621b5ffcd428b1a8417686b4.png)

### <a name="_28">;</a> 算法思路

定义一个h(n) = f(n)+g(n),即定义一个代价函数和启发函数,代价函数这里取其探索的层数,启发函数为不在位的数码数码。

f(n):探索的层数

g(n):不在位的数码个数。

从初始结点开始，扩展可能的节点，并从可能节点中选取代价最小的节点进行下一步的扩展。

```python

def get_loc(num, _arr):
    _arr = np.array(_arr).reshape(3, 3)
    for i in range(len(_arr)):
        for j in range(len(_arr[i])):
            if num == _arr[i][j]:
                return i, j

def val(arr, arr_final, method=0):
    if method == 1:
        _arr = np.array(arr).reshape(3, 3)
        _arr_final = np.array(arr_final).reshape(3, 3)
        total = 0
        for i in range(len(_arr)):
            for j in range(len(_arr[i])):
                m, n = get_loc(_arr[i][j], arr_final)
                total += np.abs(i - m) + np.abs(j - n)
        return total
    if method == 2:
        return 0

    total = []
    for i in range(len(arr)):
        if arr[i] != 0:
            total.append(arr[i] - arr_final[i])
    return len(total) - total.count(0)

def arr_swap(arr, destnation, flag=False):
    if flag:
        z_pos = arr.argmin()
        tmp = arr[destnation]
        arr[destnation] = arr[z_pos]
        arr[z_pos] = tmp
        return list(arr)

    _arr = np.array(arr.copy())
    z_pos = _arr.argmin()
    tmp = _arr[destnation]
    _arr[destnation] = _arr[z_pos]
    _arr[z_pos] = tmp
    return list(_arr)
```

```python

class node:
    par = None
    value = -1
    arr = None
    step = 0

    def __init__(self, p, val, a, s):
        self.par = p
        self.step = s
        self.value = val
        self.arr = np.array(a)

    def up(self, ss):
        if np.array(self.arr).argmin() - 3 >= 0:

            tmp = np.array(self.arr).argmin() - 3
            ar = arr_swap(self.arr, tmp)
            v = val(ar, arr_final, ss)
            v += self.step + 1
            new_node = node(p=self, val=v, a=ar, s=self.step + 1)
            return new_node
        else:
            return None

```

检查是否要扩展的节点已经生成，或者还没生成

```python

def in_open(t, openl):
    for i in openl:
        if all(i.arr == t.arr):
            if t.value < i.value:
                i.value = t.value

            return True
    return False

def in_close(t, closel, openl):
    for i in closel:
        if all(i.arr == t.arr):
            if t.value < i.value:
                i.value = t.value
                openl.append(t)

            return True
    return False
```

A*算法的主函数

```python

def Astar(arr_start, arr_final, val_method=1):
    start = node(None, val(arr_start, arr_final, val_method), arr_start, 0)

    open_l = []
    close_l = []

    open_l.append(start)
    n = start
    step = 0
    while (1):
        step += 1

        if n.up(val_method) is not None:
            tmp = n.up(val_method)
            f1 = in_open(tmp, open_l)
            f2 = in_close(tmp, close_l, open_l)
            if f1 == False and f2 == False:
                open_l.append(n.up(val_method))

        if n.down(val_method) is not None:
            tmp = n.down(val_method)
            f1 = in_open(tmp, open_l)
            f2 = in_close(tmp, close_l, open_l)
            if f1 == False and f2 == False:
                open_l.append(n.down(val_method))

        if n.left(val_method) is not None:
            tmp = n.left(val_method)
            f1 = in_open(tmp, open_l)
            f2 = in_close(tmp, close_l, open_l)
            if f1 == False and f2 == False:
                open_l.append(n.left(val_method))

        if n.right(val_method) is not None:
            tmp = n.right(val_method)
            f1 = in_open(tmp, open_l)
            f2 = in_close(tmp, close_l, open_l)
            if f1 == False and f2 == False:
                open_l.append(n.right(val_method))

        open_l.remove(n)

        if len(open_l) == 0:
            break
        close_l.append(n)

        node_v = []
        for i in open_l:
            node_v.append(i.value)

        min_posi = np.array(node_v).argmin()
        n = open_l[min_posi]
        if list(n.arr) == list(arr_final):
            break

    return start, n, step, len(close_l), len(open_l) + len(close_l)
```

对生成完的路径进行输出

```python
def print_put(n, start):
    final = []
    ptr = n
    index = []
    while ptr.par is not None:

        final.append(ptr.arr)
        index.append(ptr.step)
        ptr = ptr.par

    print(start.arr.reshape(3, 3))
    for i in range(len(final)):
        print("step: ", i + 1)
        print(np.array(final[len(final) - i - 1]).reshape(3, 3))

```

判断一个数码问题是否有解

```python
def getStatus(arr):
    sum = 0
    for i in range(len(arr)):
        for j in range(0, i):
            if arr[j] < arr[i] and arr[j] != 0:
                sum += 1

    return sum % 2
```

最终结果展示

![](https://img-blog.csdnimg.cn/img_convert/dc4a4be2d05ef8d322f354ee4ce91e4f.png)

![](https://img-blog.csdnimg.cn/img_convert/2309183d4faa7571b6a443bea0512682.png)

![](https://img-blog.csdnimg.cn/img_convert/ce1bcb743e875243e98527a41159acac.png)

## 实验二 BP网络

BP网络结构为（输入层，隐藏层，输出层）

**一、实验目的：**

理解BP神经网络的结构和原理，掌握反向传播学习算法对神经元的训练过程，了解反向传播公式。通过构建BP网络模式识别实例，熟悉前馈网络和反馈网络的原理及结构。

**二、实验原理**

BP学习算法是通过反向学习过程使误差最小，其算法过程从输出节点开始，反向地向第一隐含层(即最接近输入层的隐含层)传播由总误差引起的权值修正。BP网络不仅含有输入节点和输出节点，而且含有一层或多层隐(层)节点。输入信号先向前传递到隐节点，经过作用后，再把隐节点的输出信息传递到输出节点，最后给出输出结果。

1.针对教材例8.1，设计一个三层的BP网络结构模型，并以教材图8.5 为训练样本数据，图8.6为测试数据。

主要的代码即实现一个前馈的网络

```python

    def feedforward(self, data_index):

        self.hidden_data = [0 for i in range(self.hidden_num)]

        self.output_data = [0 for i in range(self.outpu_num)]

        for i in range(self.hidden_num):
            total = 0.0
            for j in range(len(self.train_data[0])):
                total += self.train_data[data_index][j] * self.i_h_weight[j][i]
            total += self.hidde_b[i]
            self.hidden_data[i] = self.sigmod(total)

        for i in range(self.outpu_num):
            total = 0.0
            for j in range(self.hidden_num):
                total += self.hidden_data[j] * self.h_o_weight[j][i]
            total += self.output_b[i]

            self.output_data[i] = self.sigmod(total)
        return self.output_data[0]
```

```python

    def feedback(self, MM, data_index):

        self.feedforward(data_index)

        for i in range(len(self.output_data)):

            self.error[i] = self.train_label[data_index][i] - self.output_data[i]

        for i in range(self.outpu_num):
            for j in range(self.hidden_num):

                self.h_o_weight[j][i] += MM * self.hidden_data[j] * self.error[i] * self.output_data[i] * (
                            1 - self.output_data[i])
            self.output_b[i] += MM * self.output_data[i] * (1 - self.output_data[i]) * self.error[i]

        for i in range(self.hidden_num):
            sum_ek = 0.0
            for k in range(self.outpu_num):

                sum_ek += self.h_o_weight[i][k] * self.error[k] * self.output_data[k] * (1 - self.output_data[k])
            for j in range(len(self.train_data[0])):
                self.i_h_weight[j][i] += MM * self.hidden_data[i] * (1 - self.hidden_data[i]) * \
                                         self.train_data[data_index][j] * sum_ek

            self.hidde_b[i] += MM * self.hidden_data[i] * (1 - self.hidden_data[i]) * sum_ek

```

```python

    def train(self, train_num, MM ):
        for i in tqdm(range(train_num)):

            for j in range(10):
                self.feedback(MM, j)
```

```python

    def predict(self, input_data):
        total = []
        for i in range(self.hidden_num):
            result = 0
            for j in range(len(input_data)):
                result += input_data[j] * self.i_h_weight[j][i]
            total.append(self.sigmod(result))
        final = []
        for i in range(self.outpu_num):
            r2 = 0
            for j in range(self.hidden_num):
                r2 += total[j] * self.h_o_weight[j][i]
            final.append(self.sigmod(r2))
        out_lable = []
        for i in final:
            max_arg = np.array(i).argmax()
            out_lable.append(max_arg)
        return final
```

**结果**

![](https://img-blog.csdnimg.cn/img_convert/8ee6735bb09f8b7d6a1a6880d02dad09.png)

有空再更新GA和产生式。代码在[github](https://github.com/xwyzsn/AIcode)上可以下载
